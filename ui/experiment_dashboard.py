"""
Streamlit dashboard to explore experiment outputs.

Run with:
    streamlit run ui/experiment_dashboard.py
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd
import streamlit as st

st.set_page_config(page_title="Experiment Dashboard", layout="wide")


BASE_EXPERIMENT_DIR = Path("evaluation_reports/experiments").resolve()


@dataclass
class ExperimentRun:
    experiment_name: str
    timestamp_dir: Path
    summary_csv: Path
    summary_json: Path


def discover_experiments(base_dir: Path) -> Dict[str, List[ExperimentRun]]:
    experiments: Dict[str, List[ExperimentRun]] = {}
    if not base_dir.exists():
        return experiments

    for experiment_root in sorted(base_dir.iterdir()):
        if not experiment_root.is_dir():
            continue

        runs: List[ExperimentRun] = []
        for timestamp_dir in sorted(experiment_root.iterdir(), reverse=True):
            if not timestamp_dir.is_dir():
                continue
            summary_csv = timestamp_dir / "summary.csv"
            summary_json = timestamp_dir / "summary.json"
            if summary_csv.exists():
                runs.append(
                    ExperimentRun(
                        experiment_name=experiment_root.name,
                        timestamp_dir=timestamp_dir,
                        summary_csv=summary_csv,
                        summary_json=summary_json if summary_json.exists() else None,
                    )
                )

        if runs:
            experiments[experiment_root.name] = runs

    return experiments


def load_summary(run: ExperimentRun) -> pd.DataFrame:
    df = pd.read_csv(run.summary_csv)
    # Ensure consistent column order
    desired_cols = [
        "experiment",
        "timestamp",
        "repo_name",
        "model_name",
        "prompt_id",
        "status",
        "generation_success",
        "generation_time",
        "overall_score",
        "dockerfile_score",
        "k8s_score",
        "tool_calls",
        "tokens_used",
        "repetition",
        "report_path",
    ]
    existing = [col for col in desired_cols if col in df.columns]
    remaining = [col for col in df.columns if col not in existing]
    return df[existing + remaining]


def load_summary_json(run: ExperimentRun) -> Optional[Dict]:
    if not run.summary_json or not run.summary_json.exists():
        return None
    return json.loads(run.summary_json.read_text(encoding="utf-8"))


def main() -> None:
    st.title("Experiment Dashboard")
    st.caption("Browse experiment runs generated by the evaluator/runner.")

    experiments = discover_experiments(BASE_EXPERIMENT_DIR)
    if not experiments:
        st.warning(
            f"No experiment summaries found under {BASE_EXPERIMENT_DIR}. "
            "Run `python run_experiments.py --config <file>` first."
        )
        return

    experiment_names = sorted(experiments.keys())
    selected_experiment = st.sidebar.selectbox(
        "Experiment", experiment_names, index=0
    )

    runs = experiments[selected_experiment]
    local_tz = datetime.now().astimezone().tzinfo or timezone.utc
    timestamp_labels: List[str] = []
    label_to_dir: Dict[str, str] = {}
    for run in runs:
        raw_name = run.timestamp_dir.name
        try:
            parsed_utc = datetime.strptime(raw_name, "%Y%m%d_%H%M%S").replace(tzinfo=timezone.utc)
            local_dt = parsed_utc.astimezone(local_tz)
            label = local_dt.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            label = raw_name
        timestamp_labels.append(label)
        label_to_dir[label] = raw_name

    selected_label = st.sidebar.selectbox(
        "Run Timestamp",
        timestamp_labels,
        index=0,
        help="Each timestamp corresponds to one invocation of the experiment runner.",
    )
    selected_run_dir = label_to_dir[selected_label]
    current_run = next(run for run in runs if run.timestamp_dir.name == selected_run_dir)

    st.header(f"{selected_experiment} â€“ {selected_label}")
    st.markdown(f"Reports directory: `{current_run.timestamp_dir}`")

    with st.spinner("Loading summary..."):
        summary_df = load_summary(current_run)
    st.subheader("Per-run Summary")
    st.dataframe(summary_df, width="stretch")

    if {"prompt_id", "overall_score"}.issubset(summary_df.columns):
        st.subheader("Score Summary by Prompt")
        agg = (
            summary_df.groupby("prompt_id")
            .agg(
                runs=("prompt_id", "count"),
                success_rate=("generation_success", "mean"),
                avg_overall=("overall_score", "mean"),
                avg_generation_time=("generation_time", "mean"),
            )
            .reset_index()
        )
        agg["success_rate"] = (agg["success_rate"] * 100).round(1)
        agg["avg_overall"] = agg["avg_overall"].round(2)
        agg["avg_generation_time"] = agg["avg_generation_time"].round(2)
        st.dataframe(agg, width="stretch")

    st.subheader("Run Details")
    selected_row = st.selectbox(
        "Select a run to inspect",
        summary_df.index,
        format_func=lambda idx: f"{summary_df.loc[idx, 'repo_name']} | {summary_df.loc[idx, 'model_name']} | prompt={summary_df.loc[idx, 'prompt_id']} | repetition={summary_df.loc[idx, 'repetition']}",
    )
    row = summary_df.loc[selected_row]
    st.json(row.to_dict())

    summary_json = load_summary_json(current_run)
    if summary_json:
        with st.expander("Raw summary.json"):
            st.json(summary_json)

    st.sidebar.info(
        "Click on a run to view the full report JSON under the referenced `report_path`."
    )


if __name__ == "__main__":
    main()
