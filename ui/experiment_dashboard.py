"""
Streamlit dashboard to explore experiment outputs.

Run with:
    streamlit run ui/experiment_dashboard.py
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd
import streamlit as st

st.set_page_config(page_title="Experiment Dashboard", layout="wide")


BASE_EXPERIMENT_DIR = Path("evaluation_reports/experiments").resolve()


@dataclass
class ExperimentRun:
    experiment_name: str
    timestamp_dir: Path
    summary_csv: Path
    summary_json: Path
    status_json: Optional[Path]


def discover_experiments(base_dir: Path) -> Dict[str, List[ExperimentRun]]:
    experiments: Dict[str, List[ExperimentRun]] = {}
    if not base_dir.exists():
        return experiments

    for experiment_root in sorted(base_dir.iterdir()):
        if not experiment_root.is_dir():
            continue

        runs: List[ExperimentRun] = []
        for timestamp_dir in sorted(experiment_root.iterdir(), reverse=True):
            if not timestamp_dir.is_dir():
                continue
            summary_csv = timestamp_dir / "summary.csv"
            summary_json = timestamp_dir / "summary.json"
            status_json = timestamp_dir / "status.json"
            if summary_csv.exists() or status_json.exists():
                runs.append(
                    ExperimentRun(
                        experiment_name=experiment_root.name,
                        timestamp_dir=timestamp_dir,
                        summary_csv=summary_csv,
                        summary_json=summary_json if summary_json.exists() else None,
                        status_json=status_json if status_json.exists() else None,
                    )
                )

        if runs:
            experiments[experiment_root.name] = runs

    return experiments


def load_summary(run: ExperimentRun) -> pd.DataFrame:
    if not run.summary_csv.exists():
        return pd.DataFrame()
    df = pd.read_csv(run.summary_csv)
    for col in ["generation_success", "build_success", "runtime_success"]:
        if col in df.columns:
            df[col] = df[col].replace({
                "True": True,
                "False": False,
                "true": True,
                "false": False,
                "None": None,
                "": None,
            })
    # Ensure consistent column order
    desired_cols = [
        "experiment",
        "timestamp",
        "repo_name",
        "model_name",
        "prompt_id",
        "status",
        "generation_success",
        "build_success",
        "runtime_success",
        "generation_time",
        "overall_score",
        "dockerfile_score",
        "k8s_score",
        "tool_calls",
        "tokens_used",
        "repetition",
        "report_path",
    ]
    existing = [col for col in desired_cols if col in df.columns]
    remaining = [col for col in df.columns if col not in existing]
    return df[existing + remaining]


def load_summary_json(run: ExperimentRun) -> Optional[Dict]:
    if not run.summary_json or not run.summary_json.exists():
        return None
    return json.loads(run.summary_json.read_text(encoding="utf-8"))


def load_status(run: ExperimentRun) -> Optional[Dict]:
    if not run.status_json or not run.status_json.exists():
        return None
    return json.loads(run.status_json.read_text(encoding="utf-8"))


def main() -> None:
    st.title("Experiment Dashboard")
    st.caption("Browse experiment runs generated by the evaluator/runner.")

    experiments = discover_experiments(BASE_EXPERIMENT_DIR)
    if not experiments:
        st.warning(
            f"No experiment summaries found under {BASE_EXPERIMENT_DIR}. "
            "Run `python run_experiments.py --config <file>` first."
        )
        return

    experiment_names = sorted(experiments.keys())
    selected_experiment = st.sidebar.selectbox(
        "Experiment", experiment_names, index=0
    )

    runs = experiments[selected_experiment]
    local_tz = datetime.now().astimezone().tzinfo or timezone.utc
    timestamp_labels: List[str] = []
    label_to_dir: Dict[str, str] = {}
    for run in runs:
        raw_name = run.timestamp_dir.name
        try:
            parsed_utc = datetime.strptime(raw_name, "%Y%m%d_%H%M%S").replace(tzinfo=timezone.utc)
            local_dt = parsed_utc.astimezone(local_tz)
            label = local_dt.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            label = raw_name
        timestamp_labels.append(label)
        label_to_dir[label] = raw_name

    selected_label = st.sidebar.selectbox(
        "Run Timestamp",
        timestamp_labels,
        index=0,
        help="Each timestamp corresponds to one invocation of the experiment runner.",
    )
    selected_run_dir = label_to_dir[selected_label]
    current_run = next(run for run in runs if run.timestamp_dir.name == selected_run_dir)

    st.header(f"{selected_experiment} â€“ {selected_label}")
    st.markdown(f"Reports directory: `{current_run.timestamp_dir}`")

    status_payload = load_status(current_run)
    if status_payload:
        state = status_payload.get("state")
        if state == "running":
            st.info("Experiment is still running; results will update as new runs complete.")
        elif state == "failed":
            st.error(f"Experiment run failed: {status_payload.get('error', 'unknown error')}")

    with st.spinner("Loading summary..."):
        summary_df = load_summary(current_run)
    st.subheader("Per-run Summary")
    if summary_df.empty:
        st.write("No completed runs recorded yet.")
    else:
        st.dataframe(summary_df, width="stretch")

    if not summary_df.empty and {"prompt_id", "overall_score"}.issubset(summary_df.columns):
        st.subheader("Score Summary by Prompt")
        agg_dict = {
            "runs": ("prompt_id", "count"),
            "success_rate": ("generation_success", "mean"),
            "avg_overall": ("overall_score", "mean"),
            "avg_generation_time": ("generation_time", "mean"),
        }
        if "build_success" in summary_df.columns:
            agg_dict["build_success_rate"] = ("build_success", "mean")
        if "runtime_success" in summary_df.columns:
            agg_dict["runtime_success_rate"] = ("runtime_success", "mean")

        agg = summary_df.groupby("prompt_id").agg(**agg_dict).reset_index()
        agg["success_rate"] = (agg["success_rate"] * 100).round(1)
        agg["avg_overall"] = agg["avg_overall"].round(2)
        agg["avg_generation_time"] = agg["avg_generation_time"].round(2)
        if "build_success_rate" in agg:
            agg["build_success_rate"] = (agg["build_success_rate"] * 100).round(1)
        if "runtime_success_rate" in agg:
            agg["runtime_success_rate"] = (agg["runtime_success_rate"] * 100).round(1)
        st.dataframe(agg, width="stretch")

    st.subheader("Run Details")
    if not summary_df.empty:
        selected_row = st.selectbox(
            "Select a run to inspect",
            summary_df.index,
            format_func=lambda idx: f"{summary_df.loc[idx, 'repo_name']} | {summary_df.loc[idx, 'model_name']} | prompt={summary_df.loc[idx, 'prompt_id']} | repetition={summary_df.loc[idx, 'repetition']}",
        )
        row = summary_df.loc[selected_row].copy()
        for col in ["generation_success", "build_success", "runtime_success"]:
            if col in row and pd.notna(row[col]):
                row[col] = bool(row[col])
        st.json(row.to_dict())

    summary_json = load_summary_json(current_run)
    if summary_json:
        with st.expander("Raw summary.json"):
            st.json(summary_json)

    st.sidebar.info(
        "Click on a run to view the full report JSON under the referenced `report_path`."
    )


if __name__ == "__main__":
    main()
